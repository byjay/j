#!/usr/bin/env python3
"""
conversationì˜ vocabì„ words_data.js ì‚¬ì „ì„ ì‚¬ìš©í•´ ìë™ í™•ì¥
"""
import json
import re

print("ğŸ“– ë°ì´í„° ë¡œë”© ì¤‘...")

# 1. words_data.jsì—ì„œ ë‹¨ì–´ ì‚¬ì „ ìƒì„± (ê°„ë‹¨í•œ ì •ê·œì‹ íŒŒì‹±)
with open(r'f:\genmini\japness\ë³€í™˜\JAP_BONG\js\words_data.js', 'r', encoding='utf-8') as f:
   words_content = f.read()

# ê°„ë‹¨í•˜ê²Œ japanese_word, reading, korean_meaning íŒ¨í„´ ë§¤ì¹­
pattern = r'japanese_word:\s*"([^"]+)",\s*reading:\s*"([^"]+)",.*?korean_meaning:\s*"([^"]+)"'
matches = re.findall(pattern, words_content, re.DOTALL)

word_dict = {}
for jp, reading, kr in matches:
    word_dict[jp] = {
        'word': jp,
        'read': reading,
        'mean': kr,
        'type': 'í•µì‹¬ë‹¨ì–´'
    }

print(f"âœ… {len(word_dict)}ê°œ ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ\n")

# 2. conversation.js ë¡œë“œ
with open(r'f:\genmini\japness\ë³€í™˜\JAP_BONG\js\conversation.js', 'r', encoding='utf-8') as f:
    conv_content = f.read()

start = conv_content.find('const conversationModuleData = ') + len('const conversationModuleData = ')
end = conv_content.find(';\n\nlet currentConversationCategory')
data_str = conv_content[start:end].strip()
conv_data = json.loads(data_str)

styles_part = conv_content[:conv_content.find('const conversationModuleData = ')]
functions_part = conv_content[end:]

print("ğŸ” Vocab í™•ì¥ ì¤‘...")

stats = {'total': 0, 'expanded': 0, 'added_words': 0}

# 3. ê° ëŒ€í™”ì˜ vocab í™•ì¥
for cat_key, cat_data in conv_data.items():
    for conv in cat_data.get('conversations', []):
        stats['total'] += 1
        jp_sentence = conv['question']['jp']
        original_vocab_count = len(conv['question']['vocab'])
        
        # ë¬¸ì¥ì—ì„œ ì‚¬ì „ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ ì°¾ê¸°
        found_words = []
        for word_jp, word_info in word_dict.items():
            if word_jp in jp_sentence:
                # ì´ë¯¸ vocabì— ìˆëŠ”ì§€ í™•ì¸
                already_exists = any(v['word'] == word_jp for v in conv['question']['vocab'])
                if not already_exists:
                    found_words.append(word_info.copy())
        
        if found_words:
            # ê¸°ì¡´ vocabì— ì¶”ê°€
            conv['question']['vocab'].extend(found_words)
            stats['expanded'] += 1
            stats['added_words'] += len(found_words)

print(f"\nğŸ“Š ê²°ê³¼:")
print(f"  ì´ ëŒ€í™”: {stats['total']}ê°œ")
print(f"  vocab í™•ì¥ëœ ëŒ€í™”: {stats['expanded']}ê°œ")
print(f"  ì¶”ê°€ëœ ë‹¨ì–´: {stats['added_words']}ê°œ")

# 4. íŒŒì¼ ì €ì¥
print(f"\nğŸ’¾ ì €ì¥ ì¤‘...")
cleaned_data_str = json.dumps(conv_data, ensure_ascii=False, indent=4)
final_content = styles_part + 'const conversationModuleData = ' + cleaned_data_str + functions_part

output_file = r'f:\genmini\japness\ë³€í™˜\JAP_BONG\js\conversation_ENRICHED.js'
with open(output_file, 'w', encoding='utf-8') as f:
    f.write(final_content)

print(f"âœ… ì €ì¥ ì™„ë£Œ: conversation_ENRICHED.js")
print(f"\nâš ï¸  í™•ì¸ í›„ conversation.jsë¡œ êµì²´í•˜ì„¸ìš”")
