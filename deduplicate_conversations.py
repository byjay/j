#!/usr/bin/env python3
"""
conversation.js ì¤‘ë³µ ì œê±° ìŠ¤í¬ë¦½íŠ¸
- conversation.js íŒŒì¼ì—ì„œ conversationModuleData ê°ì²´ë¥¼ íŒŒì‹±
- ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¤‘ë³µëœ ëŒ€í™” í•­ëª© ì œê±°
- ì •ë¦¬ëœ íŒŒì¼ì„ ë‹¤ì‹œ ìƒì„±
"""

import re
import json
from pathlib import Path
from collections import OrderedDict

def extract_data_object(file_path):
    """conversation.js íŒŒì¼ì—ì„œ conversationModuleData ê°ì²´ ì¶”ì¶œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # conversationModuleData ê°ì²´ì˜ ì‹œì‘ê³¼ ë ì°¾ê¸°
    start_marker = 'const conversationModuleData = {'
    end_marker = ';\n\nlet currentConversationCategory'
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("conversationModuleData ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # ë°ì´í„° ë¶€ë¶„ë§Œ ì¶”ì¶œ
    data_str = content[start_idx + len('const conversationModuleData = '):end_idx].strip()
    
    # í•¨ìˆ˜ ë¶€ë¶„ ì¶”ì¶œ (ë°ì´í„° ì´í›„ì˜ ëª¨ë“  ì½”ë“œ)
    functions_part = content[end_idx:]
    
    # ìŠ¤íƒ€ì¼ ë¶€ë¶„ ì¶”ì¶œ (ë°ì´í„° ì´ì „ì˜ ì½”ë“œ)
    styles_part = content[:start_idx]
    
    return data_str, styles_part, functions_part

def deduplicate_conversations(data):
    """ê° ì¹´í…Œê³ ë¦¬ì˜ ëŒ€í™” í•­ëª©ì—ì„œ ì¤‘ë³µ ì œê±°"""
    stats = {}
    
    for category_key, category_data in data.items():
        if 'conversations' not in category_data:
            continue
        
        conversations = category_data['conversations']
        original_count = len(conversations)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„¸íŠ¸ (ì¼ë³¸ì–´ ë¬¸ì¥ì„ í‚¤ë¡œ ì‚¬ìš©)
        seen = {}
        unique_conversations = []
        
        for conv in conversations:
            # question.jpë¥¼ ê³ ìœ  ì‹ë³„ìë¡œ ì‚¬ìš©
            jp_key = conv['question']['jp']
            
            if jp_key not in seen:
                seen[jp_key] = True
                unique_conversations.append(conv)
        
        # ì¤‘ë³µ ì œê±°ëœ ë¦¬ìŠ¤íŠ¸ë¡œ êµì²´
        category_data['conversations'] = unique_conversations
        
        removed_count = original_count - len(unique_conversations)
        stats[category_key] = {
            'original': original_count,
            'unique': len(unique_conversations),
            'removed': removed_count
        }
    
    return data, stats

def main():
    # íŒŒì¼ ê²½ë¡œ
    input_file = Path(r'f:\genmini\japness\ë³€í™˜\JAP_BONG\js\conversation.js')
    output_file = Path(r'f:\genmini\japness\ë³€í™˜\JAP_BONG\js\conversation_cleaned.js')
    
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {input_file}")
    print(f"   íŒŒì¼ í¬ê¸°: {input_file.stat().st_size:,} bytes")
    
    # 1. ë°ì´í„° ì¶”ì¶œ
    try:
        data_str, styles_part, functions_part = extract_data_object(input_file)
        print(f"âœ… conversationModuleData ê°ì²´ ì¶”ì¶œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return
    
    # 2. JSON íŒŒì‹±
    try:
        # JavaScript ê°ì²´ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬
        # ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ, trailing comma ì œê±° ë“±
        data = json.loads(data_str)
        print(f"âœ… JSON íŒŒì‹± ì™„ë£Œ")
        print(f"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(data)}")
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return
    
    # 3. ì¤‘ë³µ ì œê±°
    print(f"\nğŸ” ì¤‘ë³µ í•­ëª© ì œê±° ì¤‘...")
    cleaned_data, stats = deduplicate_conversations(data)
    
    # 4. í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì¤‘ë³µ ì œê±° í†µê³„:")
    print(f"{'ì¹´í…Œê³ ë¦¬':<20} {'ì›ë³¸':<10} {'ìœ ë‹ˆí¬':<10} {'ì œê±°ë¨':<10}")
    print("=" * 60)
    
    total_original = 0
    total_unique = 0
    total_removed = 0
    
    for category, stat in stats.items():
        print(f"{category:<20} {stat['original']:<10} {stat['unique']:<10} {stat['removed']:<10}")
        total_original += stat['original']
        total_unique += stat['unique']
        total_removed += stat['removed']
    
    print("=" * 60)
    print(f"{'í•©ê³„':<20} {total_original:<10} {total_unique:<10} {total_removed:<10}")
    
    # 5. íŒŒì¼ ì €ì¥
    print(f"\nğŸ’¾ ì •ë¦¬ëœ íŒŒì¼ ìƒì„± ì¤‘...")
    
    # JSONì„ JavaScript ê°ì²´ ë¬¸ìì—´ë¡œ ë³€í™˜
    cleaned_data_str = json.dumps(cleaned_data, ensure_ascii=False, indent=4)
    
    # ìµœì¢… íŒŒì¼ ë‚´ìš© ì¡°í•©
    final_content = (
        styles_part +
        'const conversationModuleData = ' +
        cleaned_data_str +
        functions_part
    )
    
    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(final_content)
    
    output_size = output_file.stat().st_size
    original_size = input_file.stat().st_size
    reduction = ((original_size - output_size) / original_size) * 100
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ì›ë³¸ í¬ê¸°: {original_size:,} bytes")
    print(f"   ì •ë¦¬ í›„: {output_size:,} bytes")
    print(f"   ê°ì†Œìœ¨: {reduction:.1f}%")
    
    print(f"\nâš ï¸  ë°±ì—… ê¶Œì¥: ì›ë³¸ íŒŒì¼ì„ ë°±ì—…í•œ í›„ ì •ë¦¬ëœ íŒŒì¼ë¡œ êµì²´í•˜ì„¸ìš”.")

if __name__ == '__main__':
    main()
